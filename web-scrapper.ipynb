{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = \"software developer\"\n",
    "location = \"germany\"\n",
    "page_limit = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_job_search_url(job_title, location, page_num):\n",
    "#     words = job_title.strip().split(\" \")\n",
    "#     print(len(words))\n",
    "#     print(words)\n",
    "#     if len(words) > 1: \n",
    "#         keywords = job_title.strip().replace(\" \",\"%2B\")\n",
    "#         print(keywords)\n",
    "#         url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={keywords}&location={location}&start={page_num}\"\n",
    "#         print(url)\n",
    "#     else:\n",
    "#         keywords = job_title.strip()\n",
    "#         page_num = str(page_num)\n",
    "#         url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={keywords}&location={location}&start={page_num}\"\n",
    "#         print(url)\n",
    "#     return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_search_url(job_title, location, page_num):\n",
    "    keywords = job_title.strip()\n",
    "    keywords = keywords.replace(\" \",\"%2B\")\n",
    "    page_num = str(page_num)\n",
    "    url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={keywords}&location={location}&start={page_num}\"\n",
    "    # print(url)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_job_description_url(job_id):\n",
    "    desc_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}\"\n",
    "    return desc_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Senior%2BFullstack%2BDeveloper%2B(M/F/D)%2Bat%2BPrehApp%2BGmbH&location=germany&start=0\n",
      "1\n",
      "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Senior%2BFullstack%2BDeveloper%2B(M/F/D)%2Bat%2BPrehApp%2BGmbH&location=germany&start=1\n",
      "2\n",
      "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Senior%2BFullstack%2BDeveloper%2B(M/F/D)%2Bat%2BPrehApp%2BGmbH&location=germany&start=2\n",
      "3\n",
      "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Senior%2BFullstack%2BDeveloper%2B(M/F/D)%2Bat%2BPrehApp%2BGmbH&location=germany&start=3\n",
      "4\n",
      "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Senior%2BFullstack%2BDeveloper%2B(M/F/D)%2Bat%2BPrehApp%2BGmbH&location=germany&start=4\n",
      "5\n",
      "https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Senior%2BFullstack%2BDeveloper%2B(M/F/D)%2Bat%2BPrehApp%2BGmbH&location=germany&start=5\n"
     ]
    }
   ],
   "source": [
    "page_num = 0\n",
    "while page_num <= page_limit:\n",
    "    print(page_num)\n",
    "    job_search_url = create_job_search_url(job_title, location, page_num)\n",
    "    print(job_search_url)\n",
    "    jobs_list = requests.get(job_search_url)\n",
    "    if jobs_list is not None:\n",
    "        job_list_soup = BeautifulSoup(jobs_list.text, \"html.parser\")\n",
    "        page_jobs = job_list_soup.find_all(\"li\")\n",
    "        for job in page_jobs:\n",
    "            job_div = job.find(\"div\", {\"class\":\"base-card\"})\n",
    "            job_role = job_div.find(\"h3\", {\"class\":\"base-search-card__title\"}).get_text()\n",
    "            job_location = job_div.find(\"span\", {\"class\":\"job-search-card__location\"}).get_text()\n",
    "            company_name = job_div.find(\"a\", {\"class\":\"hidden-nested-link\"}).get_text()\n",
    "            try:\n",
    "                date_posted_exact = job_div.find(\"time\", {\"class\":\"job-search-card__listdate\"}).get(\"datetime\")\n",
    "            except:\n",
    "                date_posted_exact = None\n",
    "            try:\n",
    "                date_posted_relative = job_div.find(\"time\", {\"class\":\"job-search-card__listdate\"}).get_text().strip()\n",
    "            except:\n",
    "                date_posted_relative = None    \n",
    "            job_id = job_div.get(\"data-entity-urn\").split(\":\")[3]\n",
    "            job_description_page = requests.get(create_job_description_url(job_id))\n",
    "            job_dsec_soup = BeautifulSoup(job_description_page.text, \"html.parser\")\n",
    "        page_num = page_num + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openAI\n",
      "  Downloading openai-1.70.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openAI)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openAI)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openAI)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openAI)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openAI)\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting sniffio (from openAI)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openAI)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\odin\\linkedin-jobs-application-generator\\.venv\\lib\\site-packages (from openAI) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\odin\\linkedin-jobs-application-generator\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openAI) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\odin\\linkedin-jobs-application-generator\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openAI) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\odin\\linkedin-jobs-application-generator\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openAI) (2025.1.31)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openAI)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openAI)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openAI)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.0 (from pydantic<3,>=1.9.0->openAI)\n",
      "  Downloading pydantic_core-2.33.0-cp310-cp310-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openAI)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\odin\\linkedin-jobs-application-generator\\.venv\\lib\\site-packages (from tqdm>4->openAI) (0.4.6)\n",
      "Downloading openai-1.70.0-py3-none-any.whl (599 kB)\n",
      "   ---------------------------------------- 0.0/599.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 599.1/599.1 kB 10.5 MB/s eta 0:00:00\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp310-cp310-win_amd64.whl (208 kB)\n",
      "Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "Downloading pydantic_core-2.33.0-cp310-cp310-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 12.0 MB/s eta 0:00:00\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, openAI\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 jiter-0.9.0 openAI-1.70.0 pydantic-2.11.1 pydantic-core-2.33.0 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
